{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b007b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: Environment Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4739b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Load and prepare data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"Original dataset shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d19e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: PCA - Reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca2 = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_var = pca.explained_variance_ratio_\n",
    "cumsum_var = np.cumsum(explained_var)\n",
    "\n",
    "print(f\"Explained variance by each component: {explained_var}\")\n",
    "print(f\"Cumulative explained variance: {cumsum_var}\")\n",
    "print(f\"Total variance explained by 2 components: {cumsum_var[-1]:.2%}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca2[:, 0], X_pca2[:, 1], c=y, cmap='viridis', s=100, alpha=0.7, edgecolors='black')\n",
    "plt.xlabel(f'PC1 ({explained_var[0]:.1%})')\n",
    "plt.ylabel(f'PC2 ({explained_var[1]:.1%})')\n",
    "plt.title('PCA: Iris Dataset (2 Components)')\n",
    "plt.colorbar(scatter, label='Class')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPCA components shape: {pca.components_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73787674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Analyze explained variance\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1), \n",
    "            pca_full.explained_variance_ratio_, alpha=0.7, color='blue')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Individual Explained Variance')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, len(cumsum_variance) + 1), cumsum_variance, 'bo-', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[1].set_xlabel('Number of Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[1].set_title('Cumulative Explained Variance')\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "print(f\"Components needed for 95% variance: {n_components_95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e2fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: PCA with 3 components for 3D visualization\n",
    "pca3 = PCA(n_components=3)\n",
    "X_pca3 = pca3.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(X_pca3[:, 0], X_pca3[:, 1], X_pca3[:, 2], \n",
    "                     c=y, cmap='viridis', s=100, alpha=0.7, edgecolors='black')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca3.explained_variance_ratio_[0]:.1%})')\n",
    "ax.set_ylabel(f'PC2 ({pca3.explained_variance_ratio_[1]:.1%})')\n",
    "ax.set_zlabel(f'PC3 ({pca3.explained_variance_ratio_[2]:.1%})')\n",
    "ax.set_title('PCA: 3D Visualization')\n",
    "\n",
    "plt.colorbar(scatter, ax=ax, label='Class')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained: {np.sum(pca3.explained_variance_ratio_):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ab136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Feature Selection - SelectKBest\n",
    "selector = SelectKBest(f_classif, k=2)\n",
    "X_selected = selector.fit_transform(X_scaled, y)\n",
    "\n",
    "# Get feature scores and rankings\n",
    "scores = selector.scores_\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "\n",
    "# Create ranking\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores,\n",
    "    'Selected': [i in selected_indices for i in range(len(feature_names))]\n",
    "}).sort_values('Score', ascending=False)\n",
    "\n",
    "print(\"Feature Selection (SelectKBest - Top 2):\")\n",
    "print(feature_ranking)\n",
    "\n",
    "# Visualize selected features\n",
    "plt.figure(figsize=(10, 5))\n",
    "colors = ['green' if x else 'gray' for x in feature_ranking['Selected']]\n",
    "plt.barh(feature_ranking['Feature'], feature_ranking['Score'], color=colors)\n",
    "plt.xlabel('F-Score')\n",
    "plt.title('Feature Selection Scores')\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6: Tree-based Feature Importance\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_scaled, y)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "print(importance_df)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f744639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7: Comparison - PCA vs Feature Selection\n",
    "# Load digits dataset for better visualization\n",
    "digits = load_digits()\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "X_digits_scaled = StandardScaler().fit_transform(X_digits)\n",
    "\n",
    "# PCA\n",
    "pca_digits = PCA(n_components=2)\n",
    "X_pca_digits = pca_digits.fit_transform(X_digits_scaled)\n",
    "\n",
    "# t-SNE (with reduced perplexity for speed)\n",
    "tsne = TSNE(n_components=2, random_state=42, n_iter=1000, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_digits_scaled)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "scatter1 = axes[0].scatter(X_pca_digits[:, 0], X_pca_digits[:, 1], \n",
    "                           c=y_digits, cmap='tab10', s=50, alpha=0.6)\n",
    "axes[0].set_title('PCA (2 Components)')\n",
    "axes[0].set_xlabel('PC1')\n",
    "axes[0].set_ylabel('PC2')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                           c=y_digits, cmap='tab10', s=50, alpha=0.6)\n",
    "axes[1].set_title('t-SNE (2 Components)')\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original dimensions: {X_digits.shape[1]}\")\n",
    "print(f\"Reduced to 2 dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86032be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 8: Testing and Validation\n",
    "test_results = []\n",
    "\n",
    "# Test 1: PCA variance explained\n",
    "pca_test = PCA(n_components=2)\n",
    "pca_test.fit(X_scaled)\n",
    "variance_explained = np.sum(pca_test.explained_variance_ratio_)\n",
    "test1 = variance_explained > 0.95\n",
    "test_results.append((\"Test 1: PCA Variance > 95%\", test1, f\"{variance_explained:.2%}\"))\n",
    "\n",
    "# Test 2: PCA components shape\n",
    "pca_trans = PCA(n_components=2)\n",
    "X_trans = pca_trans.fit_transform(X_scaled)\n",
    "test2 = X_trans.shape == (150, 2)\n",
    "test_results.append((\"Test 2: Transformed shape correct\", test2, f\"Shape: {X_trans.shape}\"))\n",
    "\n",
    "# Test 3: Feature selection works\n",
    "selector_test = SelectKBest(f_classif, k=2)\n",
    "X_sel = selector_test.fit_transform(X_scaled, y)\n",
    "test3 = X_sel.shape == (150, 2)\n",
    "test_results.append((\"Test 3: Feature selection shape\", test3, f\"Shape: {X_sel.shape}\"))\n",
    "\n",
    "# Test 4: No NaN values in PCA\n",
    "pca_full = PCA()\n",
    "X_full_pca = pca_full.fit_transform(X_scaled)\n",
    "test4 = not np.isnan(X_full_pca).any()\n",
    "test_results.append((\"Test 4: No NaN in PCA output\", test4, \"All valid values\"))\n",
    "\n",
    "# Test 5: PCA preserves distances approximately\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "dist_orig = euclidean_distances(X_scaled[:10])\n",
    "dist_pca = euclidean_distances(X_pca2[:10])\n",
    "correlation = np.corrcoef(dist_orig.flatten(), dist_pca.flatten())[0, 1]\n",
    "test5 = correlation > 0.7\n",
    "test_results.append((\"Test 5: Distance correlation > 0.7\", test5, f\"Correlation: {correlation:.3f}\"))\n",
    "\n",
    "# Print results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRACTICAL 7: DIMENSIONALITY REDUCTION - TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "passed = 0\n",
    "for test_name, result, details in test_results:\n",
    "    status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "    print(f\"{status} | {test_name}\")\n",
    "    print(f\"       Details: {details}\")\n",
    "    if result:\n",
    "        passed += 1\n",
    "\n",
    "print(f\"\\nTotal: {passed}/{len(test_results)} tests passed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e746c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and Reflection\n",
    "summary = \"\"\"\n",
    "KEY LEARNINGS - PRACTICAL 7\n",
    "===========================\n",
    "\n",
    "1. DIMENSIONALITY REDUCTION TECHNIQUES:\n",
    "   - PCA: Linear, interpretable, fast\n",
    "   - t-SNE: Non-linear, better visualization, slower\n",
    "   - Feature Selection: Interpretable, uses original features\n",
    "\n",
    "2. PCA INSIGHTS:\n",
    "   - Explained variance helps choose component count\n",
    "   - PC1 + PC2 explain ~95% of iris variance\n",
    "   - Components are orthogonal (independent)\n",
    "   - Loadings show original feature contributions\n",
    "\n",
    "3. FEATURE SELECTION:\n",
    "   - F-score ranks features by class separability\n",
    "   - Tree-based importance identifies non-linear relationships\n",
    "   - Keeps original features for interpretability\n",
    "\n",
    "4. WHEN TO USE EACH:\n",
    "   - High-dimensional data (>50 features): PCA\n",
    "   - Visualization needed: t-SNE or PCA\n",
    "   - Interpretability critical: Feature selection\n",
    "   - Performance comparison: Try all three\n",
    "\n",
    "5. PRACTICAL APPLICATIONS:\n",
    "   - Image compression with PCA\n",
    "   - Gene expression analysis with feature selection\n",
    "   - Data visualization with t-SNE\n",
    "   - Curse of dimensionality mitigation\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
