# PRACTICAL 7: Dimensionality Reduction

## Course Information
- **Course Code:** 316316
- **Course Title:** Machine Learning
- **Semester:** 6th
- **Practical Duration:** 2 Hours
- **Learning Outcome Code:** LLO 7.1
- **Aligned Course Outcomes:** CO3, CO4

---

## Learning Outcomes (Practical Level)

After completing this practical, students will be able to:

1. **Understand dimensionality reduction** techniques and benefits
2. **Implement PCA and other reduction methods**
3. **Reduce feature dimensionality** while preserving information
4. **Improve model performance** with reduced features
5. **Visualize high-dimensional data** in lower dimensions

---

## Practical Objective

This practical teaches dimensionality reduction techniques used to:
- Reduce computational complexity
- Remove multicollinearity
- Improve model generalization
- Handle curse of dimensionality
- Visualize high-dimensional data

---

## Prerequisites

- Completion of Practicals 1-6
- Understanding of linear algebra
- Knowledge of variance and covariance

---

## Topics Covered

1. **Principal Component Analysis (PCA)**
   - Eigenvectors and eigenvalues
   - Variance explanation
   - Selecting number of components
   - PCA on standardized data

2. **Feature Selection Methods**
   - Filter methods (correlation, variance)
   - Wrapper methods (RFE, forward/backward selection)
   - Embedded methods (L1/L2 regularization)

3. **Linear Discriminant Analysis (LDA)**
   - Class separability
   - Between-class and within-class variance
   - LDA for classification and reduction

4. **t-SNE**
   - Non-linear reduction
   - Perplexity parameter
   - Visualization of clusters
   - t-SNE for exploratory analysis

5. **UMAP**
   - Uniform Manifold Approximation and Projection
   - Preserving local and global structure
   - Applications in clustering visualization

6. **Autoencoders**
   - Neural network-based reduction
   - Bottleneck layer
   - Learning representations

7. **Practical Applications**
   - Curse of dimensionality
   - Feature importance in ML models
   - Trade-offs between reduction and information

---

## Learning Resources

- **Libraries:** Scikit-learn, TensorFlow, umap-learn
- **Visualization:** Matplotlib, Plotly
- **Datasets:** High-dimensional datasets (digits, MNIST)

---

## Assessment Criteria

- Correct implementation of reduction techniques
- Appropriate method selection
- Variance preservation analysis
- Visualization quality
- Performance comparison (with/without reduction)
- Code documentation

---

## Next Steps

After completing this practical:
- Move to Practical 8: Model evaluation and validation
- Apply to feature engineering pipelines
- Explore manifold learning
