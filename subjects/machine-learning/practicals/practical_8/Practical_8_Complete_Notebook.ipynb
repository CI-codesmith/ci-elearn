{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8d761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 0: Setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, learning_curve, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e9fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Load data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "print(f\"Dataset: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef04ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: K-Fold Cross-Validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean: {scores.mean():.3f} (+/- {scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: Grid Search\n",
    "param_grid = {'C': [0.1, 1, 10], 'max_iter': [100, 1000]}\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_scaled, y)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"\\nResults DataFrame:\")\n",
    "results_df = pd.DataFrame(grid_search.cv_results_).iloc[:, [6, 7, 8, 9]]\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c71b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: SVM Grid Search\n",
    "svm_param_grid = {'C': [1, 10, 100], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "svm_grid = GridSearchCV(SVC(), svm_param_grid, cv=5, n_jobs=-1)\n",
    "svm_grid.fit(X_scaled, y)\n",
    "print(f\"Best SVM parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best SVM score: {svm_grid.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 5: Random Search\n",
    "from scipy.stats import randint\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    {'n_estimators': randint(10, 200), 'max_depth': randint(5, 20)},\n",
    "    n_iter=10, cv=5, random_state=42\n",
    ")\n",
    "random_search.fit(X_scaled, y)\n",
    "print(f\"Best RF parameters: {random_search.best_params_}\")\n",
    "print(f\"Best RF score: {random_search.best_score_:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4750b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 6: Learning Curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    SVC(), X_scaled, y, cv=5, scoring='accuracy',\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='Training score')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2)\n",
    "plt.plot(train_sizes, test_mean, 's-', label='Test score')\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.2)\n",
    "plt.xlabel('Training set size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392764ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 7: Comparison\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000)),\n",
    "    (\"SVM\", SVC()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"KNN\", KNeighborsClassifier())\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, model in models:\n",
    "    scores = cross_val_score(model, X_scaled, y, cv=5, scoring='accuracy')\n",
    "    results.append({'Model': name, 'Mean': scores.mean(), 'Std': scores.std()})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(results_df['Model'], results_df['Mean'], yerr=results_df['Std'], \n",
    "             fmt='o', markersize=8, capsize=5)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Comparison (5-Fold CV)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e872d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 8: Testing and Validation\n",
    "test_results = []\n",
    "\n",
    "# Test 1: Cross-validation works\n",
    "cv_model = LogisticRegression()\n",
    "cv_scores = cross_val_score(cv_model, X_scaled, y, cv=5)\n",
    "test1 = len(cv_scores) == 5 and cv_scores.min() > 0\n",
    "test_results.append((\"Test 1: 5-Fold CV\", test1, f\"Scores: {cv_scores}\"))\n",
    "\n",
    "# Test 2: Grid search finds best params\n",
    "gs = GridSearchCV(LogisticRegression(max_iter=1000), {'C': [0.1, 1, 10]}, cv=3)\n",
    "gs.fit(X_scaled, y)\n",
    "test2 = hasattr(gs, 'best_params_') and hasattr(gs, 'best_score_')\n",
    "test_results.append((\"Test 2: Grid Search\", test2, f\"Best C: {gs.best_params_['C']}\"))\n",
    "\n",
    "# Test 3: Multiple models compared\n",
    "test_models = [LogisticRegression(), SVC(), RandomForestClassifier()]\n",
    "test_scores = [cross_val_score(m, X_scaled, y, cv=3).mean() for m in test_models]\n",
    "test3 = len(test_scores) == 3 and all(s > 0 for s in test_scores)\n",
    "test_results.append((\"Test 3: Model Comparison\", test3, f\"Scores: {[f'{s:.2f}' for s in test_scores]}\"))\n",
    "\n",
    "# Test 4: Learning curve data valid\n",
    "test4 = len(train_mean) == len(test_mean) and all(train_mean >= 0)\n",
    "test_results.append((\"Test 4: Learning Curves\", test4, f\"Curves length: {len(train_mean)}\"))\n",
    "\n",
    "# Test 5: Best model from grid search\n",
    "best_model = gs.best_estimator_\n",
    "best_score = gs.score(X_scaled, y)\n",
    "test5 = best_score > 0.8 and isinstance(best_model, LogisticRegression)\n",
    "test_results.append((\"Test 5: Best Model Score\", test5, f\"Score: {best_score:.3f}\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRACTICAL 8: MODEL EVALUATION - TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "passed = 0\n",
    "for test_name, result, details in test_results:\n",
    "    status = \"✅ PASS\" if result else \"❌ FAIL\"\n",
    "    print(f\"{status} | {test_name}\")\n",
    "    print(f\"       Details: {details}\")\n",
    "    if result:\n",
    "        passed += 1\n",
    "\n",
    "print(f\"\\nTotal: {passed}/{len(test_results)} tests passed\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64012868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PRACTICAL 8 COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
