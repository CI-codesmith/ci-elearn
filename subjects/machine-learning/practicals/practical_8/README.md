# PRACTICAL 8: Model Evaluation and Validation

## Course Information
- **Course Code:** 316316
- **Course Title:** Machine Learning
- **Semester:** 6th
- **Practical Duration:** 2 Hours
- **Learning Outcome Code:** LLO 8.1, LLO 8.2
- **Aligned Course Outcomes:** CO4

---

## Learning Outcomes (Practical Level)

After completing this practical, students will be able to:

1. **Understand evaluation metrics** for different problem types
2. **Implement proper validation strategies**
3. **Detect overfitting and underfitting**
4. **Compare models objectively**
5. **Handle imbalanced datasets** in evaluation

---

## Practical Objective

This practical covers comprehensive model evaluation and validation techniques. Students will learn:
- Different metrics for classification, regression, clustering
- Cross-validation strategies
- Hyperparameter tuning methods
- Bias-variance trade-off
- Proper train-test-validation splits

---

## Prerequisites

- Completion of Practicals 1-7
- Understanding of model performance concepts
- Knowledge of probability and statistics basics

---

## Topics Covered

1. **Classification Metrics**
   - Confusion matrix components
   - Accuracy, Precision, Recall, F1-Score
   - Specificity and Sensitivity
   - ROC curves and AUC
   - PR curves and threshold tuning

2. **Regression Metrics**
   - MSE, RMSE, MAE
   - R-squared (R²)
   - Adjusted R²
   - Mean Absolute Percentage Error (MAPE)

3. **Clustering Metrics**
   - Intra-cluster distance
   - Inter-cluster distance
   - Silhouette coefficient
   - Davies-Bouldin Index
   - Calinski-Harabasz Index

4. **Cross-Validation Techniques**
   - K-Fold cross-validation
   - Stratified K-Fold
   - Time series cross-validation
   - Leave-One-Out (LOO) cross-validation

5. **Hyperparameter Tuning**
   - Grid Search
   - Random Search
   - Bayesian optimization
   - Early stopping

6. **Bias-Variance Analysis**
   - Understanding trade-off
   - Learning curves
   - Validation curves
   - Detecting overfitting/underfitting

7. **Handling Imbalanced Data**
   - Resampling techniques (oversampling, undersampling)
   - SMOTE
   - Class weights
   - Stratified splits

---

## Learning Resources

- **Library:** Scikit-learn, Scipy
- **Visualization:** Matplotlib, Seaborn
- **Tools:** Cross-validation, GridSearchCV

---

## Assessment Criteria

- Correct metric selection
- Proper validation methodology
- Accurate performance reporting
- Hyperparameter tuning effectiveness
- Code documentation
- Result interpretation

---

## Next Steps

After completing this practical:
- Move to Practical 9: Feature extraction and transformation
- Apply evaluation to micro-projects
- Explore AutoML techniques
