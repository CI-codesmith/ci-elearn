# PRACTICAL 11: Advanced Optimization Techniques

## Course Information
- **Course Code:** 316316
- **Course Title:** Machine Learning
- **Semester:** 6th
- **Practical Duration:** 2 Hours
- **Learning Outcome Code:** LLO 11.1
- **Aligned Course Outcomes:** CO4, CO5

---

## Learning Outcomes (Practical Level)

After completing this practical, students will be able to:

1. **Understand optimization algorithms** used in ML
2. **Implement gradient descent variants**
3. **Apply learning rate scheduling**
4. **Optimize hyperparameters** effectively
5. **Handle convergence issues** in optimization

---

## Practical Objective

This practical covers optimization techniques essential for training ML models. Students will learn:
- Different optimization algorithms
- Learning rate and convergence
- Momentum and acceleration techniques
- Adaptive learning rate methods
- Constraint and regularized optimization

---

## Prerequisites

- Completion of Practicals 1-10
- Understanding of calculus and gradients
- Knowledge of loss functions

---

## Topics Covered

1. **Gradient Descent Variants**
   - Batch Gradient Descent
   - Stochastic Gradient Descent (SGD)
   - Mini-batch Gradient Descent
   - Learning rate effects

2. **Momentum-Based Methods**
   - Momentum optimization
   - Nesterov Accelerated Gradient (NAG)
   - Convergence acceleration
   - Parameter tuning

3. **Adaptive Learning Rate Methods**
   - AdaGrad algorithm
   - RMSprop algorithm
   - Adam optimizer
   - AdamW with weight decay

4. **Learning Rate Scheduling**
   - Step decay
   - Exponential decay
   - Polynomial decay
   - Cyclical learning rates
   - Warm restarts

5. **Regularization in Optimization**
   - L1 and L2 regularization
   - Elastic Net
   - Dropout
   - Early stopping

6. **Constraint-Based Optimization**
   - L-BFGS algorithm
   - Coordinate descent
   - Proximal methods

7. **Bayesian Optimization**
   - Gaussian processes
   - Acquisition functions
   - HyperOpt library
   - Hyperparameter tuning

---

## Learning Resources

- **Libraries:** Scikit-learn, TensorFlow, PyTorch, Optuna
- **Visualization:** Loss curves, learning rate effects
- **Tools:** Optimization algorithms, schedulers

---

## Assessment Criteria

- Understanding of optimization theory
- Correct algorithm implementation
- Convergence analysis
- Learning rate selection
- Performance optimization
- Code documentation

---

## Next Steps

After completing this practical:
- Move to Practical 12: Time series analysis
- Work on neural network optimization
- Explore distributed optimization
