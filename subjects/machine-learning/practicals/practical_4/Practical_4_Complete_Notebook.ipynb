{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49df58b8",
   "metadata": {},
   "source": [
    "## Student Information\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Roll Number:** [Your Roll Number]  \n",
    "**Date:** [Submission Date]  \n",
    "**College:** [Your College Name]  \n",
    "**Academic Year:** 2025-2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Outcomes Checklist\n",
    "\n",
    "- [ ] LO1: Split data into train and test sets appropriately\n",
    "- [ ] LO2: Implement Decision Tree Classifier\n",
    "- [ ] LO3: Implement K-Nearest Neighbors (KNN)\n",
    "- [ ] LO4: Implement Logistic Regression\n",
    "- [ ] LO5: Evaluate using confusion matrix and classification reports\n",
    "- [ ] LO6: Calculate precision, recall, F1-score metrics\n",
    "- [ ] LO7: Compare multiple classifiers\n",
    "- [ ] LO8: Tune hyperparameters\n",
    "- [ ] LO9: Visualize model performance\n",
    "- [ ] LO10: Select best model based on metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c7648",
   "metadata": {},
   "source": [
    "## Phase 0: Environment Setup & Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (confusion_matrix, classification_report, \n",
    "                             accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, roc_curve, auc)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Print versions\n",
    "print(\"ðŸ“š LIBRARY VERSIONS:\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Scikit-learn: {__import__('sklearn').__version__}\")\n",
    "print(\"\\nâœ… All libraries imported successfully!\")\n",
    "\n",
    "# Configure display\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f9bfb5",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preparation & Exploration\n",
    "\n",
    "Prepare a classification dataset for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classification dataset\n",
    "from sklearn.datasets import make_classification, load_iris\n",
    "\n",
    "print(\"ðŸ“Š CREATING CLASSIFICATION DATASET:\\n\")\n",
    "\n",
    "# Use Iris dataset for classification\n",
    "X, y = load_iris(return_X_y=True)\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "target_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['Species'] = y\n",
    "df['Species_Name'] = df['Species'].map({i: target_names[i] for i in range(3)})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFeatures: {feature_names}\")\n",
    "print(f\"Target classes: {target_names}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['Species_Name'].value_counts())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3855e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"ðŸ“ˆ DATA EXPLORATION:\\n\")\n",
    "\n",
    "print(f\"Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca3689",
   "metadata": {},
   "source": [
    "## Phase 2: Train-Test Split\n",
    "\n",
    "Split data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "print(\"ðŸ”€ TRAIN-TEST SPLIT:\\n\")\n",
    "\n",
    "X = df[feature_names]\n",
    "y = df['Species']\n",
    "\n",
    "# Split data: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(f\"\\nTesting set distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())\n",
    "print(f\"\\nâœ… Data split successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d0fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (important for KNN and Logistic Regression)\n",
    "print(\"\\nðŸ“ FEATURE SCALING:\\n\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data - Mean: {X_train_scaled.mean(axis=0):.2f}\")\n",
    "print(f\"Training data - Std: {X_train_scaled.std(axis=0):.2f}\")\n",
    "print(f\"\\nâœ… Features scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea530c53",
   "metadata": {},
   "source": [
    "## Phase 3: Decision Tree Classifier\n",
    "\n",
    "Implement and evaluate Decision Tree for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd670a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree\n",
    "print(\"ðŸŒ³ DECISION TREE CLASSIFIER:\\n\")\n",
    "\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=5,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
    "print(f\"âœ… Decision Tree trained!\")\n",
    "print(f\"Accuracy: {dt_accuracy:.4f} ({dt_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  - Max depth: 5\")\n",
    "print(f\"  - Min samples split: 2\")\n",
    "print(f\"  - Min samples leaf: 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70d463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Decision Tree\n",
    "print(\"\\nðŸ“Š FEATURE IMPORTANCE:\\n\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.title('Feature Importance - Decision Tree', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Feature importance calculated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebac87a",
   "metadata": {},
   "source": [
    "## Phase 4: K-Nearest Neighbors (KNN)\n",
    "\n",
    "Implement and evaluate KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac12aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN model\n",
    "print(\"ðŸ‘¥ K-NEAREST NEIGHBORS (KNN):\\n\")\n",
    "\n",
    "knn_model = KNeighborsClassifier(\n",
    "    n_neighbors=5,\n",
    "    weights='uniform',\n",
    "    metric='euclidean'\n",
    ")\n",
    "\n",
    "# Train on scaled data\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "print(f\"âœ… KNN trained!\")\n",
    "print(f\"Accuracy: {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  - K neighbors: 5\")\n",
    "print(f\"  - Weights: uniform\")\n",
    "print(f\"  - Metric: euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c31aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K\n",
    "print(\"\\nðŸ” FINDING OPTIMAL K:\\n\")\n",
    "\n",
    "k_values = range(1, 16)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_scores.append(knn.score(X_train_scaled, y_train))\n",
    "    test_scores.append(knn.score(X_test_scaled, y_test))\n",
    "\n",
    "# Plot K vs Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, train_scores, marker='o', label='Train Accuracy')\n",
    "plt.plot(k_values, test_scores, marker='s', label='Test Accuracy')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN: Effect of K on Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(test_scores)]\n",
    "print(f\"Optimal K: {best_k}\")\n",
    "print(f\"Best test accuracy: {max(test_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fdcda",
   "metadata": {},
   "source": [
    "## Phase 5: Logistic Regression\n",
    "\n",
    "Implement and evaluate Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b01f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"ðŸ“ˆ LOGISTIC REGRESSION:\\n\")\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=200,\n",
    "    multi_class='multinomial',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train on scaled data\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "print(f\"âœ… Logistic Regression trained!\")\n",
    "print(f\"Accuracy: {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "print(f\"  - Max iterations: 200\")\n",
    "print(f\"  - Multi-class: multinomial\")\n",
    "print(f\"  - Solver: lbfgs (default)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficient analysis\n",
    "print(\"\\nðŸ“Š COEFFICIENT ANALYSIS:\\n\")\n",
    "\n",
    "coefficients = pd.DataFrame(\n",
    "    lr_model.coef_,\n",
    "    columns=feature_names,\n",
    "    index=target_names\n",
    ")\n",
    "\n",
    "print(\"Logistic Regression Coefficients:\")\n",
    "print(coefficients)\n",
    "print(f\"\\nIntercept (bias): {lr_model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2f317",
   "metadata": {},
   "source": [
    "## Phase 6: Model Evaluation\n",
    "\n",
    "Comprehensive evaluation of all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6310570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "print(\"ðŸŽ¯ CONFUSION MATRICES:\\n\")\n",
    "\n",
    "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
    "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(cm_dt)\n",
    "print(\"\\nKNN Confusion Matrix:\")\n",
    "print(cm_knn)\n",
    "print(\"\\nLogistic Regression Confusion Matrix:\")\n",
    "print(cm_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "matrices = [cm_dt, cm_knn, cm_lr]\n",
    "titles = ['Decision Tree', 'KNN', 'Logistic Regression']\n",
    "\n",
    "for ax, cm, title in zip(axes, matrices, titles):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Confusion matrices visualized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38217266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports\n",
    "print(\"ðŸ“‹ CLASSIFICATION REPORTS:\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DECISION TREE\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_dt, target_names=target_names))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"K-NEAREST NEIGHBORS\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_knn, target_names=target_names))\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred_lr, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ec56b",
   "metadata": {},
   "source": [
    "## Phase 7: Performance Metrics Comparison\n",
    "\n",
    "Compare all models using multiple evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e0e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate all metrics\n",
    "print(\"ðŸ“Š PERFORMANCE METRICS COMPARISON:\\n\")\n",
    "\n",
    "metrics_data = []\n",
    "\n",
    "# Decision Tree\n",
    "metrics_data.append({\n",
    "    'Model': 'Decision Tree',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
    "    'Precision': precision_score(y_test, y_pred_dt, average='weighted'),\n",
    "    'Recall': recall_score(y_test, y_pred_dt, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, y_pred_dt, average='weighted')\n",
    "})\n",
    "\n",
    "# KNN\n",
    "metrics_data.append({\n",
    "    'Model': 'KNN (k=5)',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_knn),\n",
    "    'Precision': precision_score(y_test, y_pred_knn, average='weighted'),\n",
    "    'Recall': recall_score(y_test, y_pred_knn, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, y_pred_knn, average='weighted')\n",
    "})\n",
    "\n",
    "# Logistic Regression\n",
    "metrics_data.append({\n",
    "    'Model': 'Logistic Regression',\n",
    "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
    "    'Precision': precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'Recall': recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "    'F1-Score': f1_score(y_test, y_pred_lr, average='weighted')\n",
    "})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(metrics_df))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, metrics_df['Accuracy'], width, label='Accuracy')\n",
    "ax.bar(x - 0.5*width, metrics_df['Precision'], width, label='Precision')\n",
    "ax.bar(x + 0.5*width, metrics_df['Recall'], width, label='Recall')\n",
    "ax.bar(x + 1.5*width, metrics_df['F1-Score'], width, label='F1-Score')\n",
    "\n",
    "ax.set_xlabel('Models', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title('Classification Models - Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics_df['Model'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0.8, 1.05])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Metrics comparison visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f314919e",
   "metadata": {},
   "source": [
    "## Phase 8: Practical Tests\n",
    "\n",
    "Complete all 5 tests to verify your learning outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4e688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: TRAIN-TEST SPLIT\n",
    "print(\"ðŸ§ª TEST 1: TRAIN-TEST SPLIT\")\n",
    "try:\n",
    "    assert X_train.shape[0] > 0, \"Training set is empty!\"\n",
    "    assert X_test.shape[0] > 0, \"Test set is empty!\"\n",
    "    assert X_train.shape[1] == X_test.shape[1], \"Feature mismatch!\"\n",
    "    assert len(y_train) == X_train.shape[0], \"Target mismatch!\"\n",
    "    print(f\"âœ… TEST 1 PASSED: Train size {X_train.shape[0]}, Test size {X_test.shape[0]}\")\n",
    "    test1_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ TEST 1 FAILED: {e}\")\n",
    "    test1_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: DECISION TREE\n",
    "print(\"\\nðŸ§ª TEST 2: DECISION TREE CLASSIFIER\")\n",
    "try:\n",
    "    assert dt_accuracy > 0.7, \"Decision Tree accuracy too low!\"\n",
    "    assert len(y_pred_dt) == len(y_test), \"Prediction size mismatch!\"\n",
    "    assert len(dt_model.feature_importances_) == 4, \"Feature importance size mismatch!\"\n",
    "    print(f\"âœ… TEST 2 PASSED: Accuracy {dt_accuracy:.4f}\")\n",
    "    test2_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ TEST 2 FAILED: {e}\")\n",
    "    test2_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51781af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: KNN CLASSIFIER\n",
    "print(\"\\nðŸ§ª TEST 3: KNN CLASSIFIER\")\n",
    "try:\n",
    "    assert knn_accuracy > 0.7, \"KNN accuracy too low!\"\n",
    "    assert len(y_pred_knn) == len(y_test), \"Prediction size mismatch!\"\n",
    "    assert knn_model.n_neighbors == 5, \"K value incorrect!\"\n",
    "    print(f\"âœ… TEST 3 PASSED: Accuracy {knn_accuracy:.4f}\")\n",
    "    test3_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ TEST 3 FAILED: {e}\")\n",
    "    test3_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 4: LOGISTIC REGRESSION\n",
    "print(\"\\nðŸ§ª TEST 4: LOGISTIC REGRESSION\")\n",
    "try:\n",
    "    assert lr_accuracy > 0.7, \"Logistic Regression accuracy too low!\"\n",
    "    assert len(y_pred_lr) == len(y_test), \"Prediction size mismatch!\"\n",
    "    assert lr_model.coef_.shape[0] == 3, \"Coefficient shape incorrect!\"\n",
    "    print(f\"âœ… TEST 4 PASSED: Accuracy {lr_accuracy:.4f}\")\n",
    "    test4_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ TEST 4 FAILED: {e}\")\n",
    "    test4_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c18f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 5: METRICS COMPARISON\n",
    "print(\"\\nðŸ§ª TEST 5: METRICS COMPARISON\")\n",
    "try:\n",
    "    assert len(metrics_df) == 3, \"Should have 3 models!\"\n",
    "    assert all(metrics_df['Accuracy'] > 0.7), \"All accuracies should be > 0.7!\"\n",
    "    assert all((metrics_df['Precision'] >= 0) & (metrics_df['Precision'] <= 1)), \"Invalid precision!\"\n",
    "    assert all((metrics_df['Recall'] >= 0) & (metrics_df['Recall'] <= 1)), \"Invalid recall!\"\n",
    "    assert all((metrics_df['F1-Score'] >= 0) & (metrics_df['F1-Score'] <= 1)), \"Invalid F1-Score!\"\n",
    "    print(f\"âœ… TEST 5 PASSED: All metrics valid and calculated\")\n",
    "    test5_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ TEST 5 FAILED: {e}\")\n",
    "    test5_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06eca8e",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1e7c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "test_summary = pd.DataFrame([\n",
    "    {'Test': 'Test 1: Train-Test Split', 'Result': test1_result},\n",
    "    {'Test': 'Test 2: Decision Tree', 'Result': test2_result},\n",
    "    {'Test': 'Test 3: KNN', 'Result': test3_result},\n",
    "    {'Test': 'Test 4: Logistic Regression', 'Result': test4_result},\n",
    "    {'Test': 'Test 5: Metrics Comparison', 'Result': test5_result}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(test_summary.to_string(index=False))\n",
    "\n",
    "passed = sum([1 for r in [test1_result, test2_result, test3_result, test4_result, test5_result] if r == 'PASSED'])\n",
    "total = 5\n",
    "print(f\"\\nðŸ“Š SCORE: {passed}/{total} TESTS PASSED ({passed*100/total:.0f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfc0ecf",
   "metadata": {},
   "source": [
    "## Reflections & Learnings\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Decision Trees:**\n",
    "   - Interpretable but prone to overfitting\n",
    "   - No feature scaling needed\n",
    "   - Good for identifying important features\n",
    "\n",
    "2. **K-Nearest Neighbors:**\n",
    "   - Simple but computationally expensive\n",
    "   - Requires feature scaling\n",
    "   - K value significantly affects performance\n",
    "\n",
    "3. **Logistic Regression:**\n",
    "   - Fast and interpretable\n",
    "   - Assumes linear decision boundary\n",
    "   - Provides probability estimates\n",
    "\n",
    "4. **Model Evaluation:**\n",
    "   - No single metric tells the whole story\n",
    "   - Confusion matrix reveals specific error types\n",
    "   - Precision vs Recall trade-off is important\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always split data into train/test sets\n",
    "- Scale features for distance-based algorithms\n",
    "- Use stratified split for imbalanced datasets\n",
    "- Compare multiple models before choosing one\n",
    "- Consider business context when selecting metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d203477",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before submitting your practical, verify:\n",
    "\n",
    "### Code Completion\n",
    "- [ ] Phase 1 (Data Exploration): Completed\n",
    "- [ ] Phase 2 (Train-Test Split): Completed with scaling\n",
    "- [ ] Phase 3 (Decision Tree): Completed with hyperparameters\n",
    "- [ ] Phase 4 (KNN): Completed with K optimization\n",
    "- [ ] Phase 5 (Logistic Regression): Completed with coefficient analysis\n",
    "- [ ] Phase 6 (Evaluation): Confusion matrices and reports\n",
    "- [ ] Phase 7 (Metrics Comparison): All metrics calculated\n",
    "\n",
    "### Test Results\n",
    "- [ ] All 5 tests passed âœ…\n",
    "- [ ] Test output clearly visible\n",
    "- [ ] Error handling demonstrated\n",
    "- [ ] Performance metrics recorded\n",
    "\n",
    "### Documentation\n",
    "- [ ] Student information filled\n",
    "- [ ] Learning outcomes checklist marked\n",
    "- [ ] Reflections written\n",
    "- [ ] Code is well-commented\n",
    "- [ ] All output is visible and clear\n",
    "\n",
    "### Files\n",
    "- [ ] Notebook saved as `Practical_4_Complete_Notebook.ipynb`\n",
    "- [ ] PDF exported from notebook\n",
    "- [ ] No errors in notebook execution\n",
    "\n",
    "---\n",
    "\n",
    "**Practical Status:** Ready for submission âœ…"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
