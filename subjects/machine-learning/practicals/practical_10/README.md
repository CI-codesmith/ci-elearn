# PRACTICAL 10: Ensemble Learning Methods

## Course Information
- **Course Code:** 316316
- **Course Title:** Machine Learning
- **Semester:** 6th
- **Practical Duration:** 2 Hours
- **Learning Outcome Code:** LLO 10.1, LLO 10.2
- **Aligned Course Outcomes:** CO4, CO5

---

## Learning Outcomes (Practical Level)

After completing this practical, students will be able to:

1. **Understand ensemble learning** concepts and benefits
2. **Implement bagging, boosting, and stacking** methods
3. **Combine multiple models** effectively
4. **Improve model robustness** through ensembles
5. **Select appropriate ensemble** techniques

---

## Practical Objective

This practical covers ensemble learning techniques that combine multiple models for better predictions. Students will learn:
- Bagging and bootstrap aggregating
- Boosting algorithms (AdaBoost, Gradient Boosting)
- Stacking and blending
- Voting ensembles
- Diversity in ensemble methods

---

## Prerequisites

- Completion of Practicals 1-9
- Understanding of base learners
- Knowledge of weighted averages

---

## Topics Covered

1. **Bagging Methods**
   - Bootstrap aggregating concept
   - Random Forest
   - Random Forest Regression
   - Feature importance from bagging

2. **Boosting Methods**
   - AdaBoost algorithm
   - Gradient Boosting
   - XGBoost (Extreme Gradient Boosting)
   - LightGBM and CatBoost
   - Sequential error correction

3. **Voting Ensembles**
   - Hard voting (majority class)
   - Soft voting (probability average)
   - Weighted voting
   - Combining different algorithms

4. **Stacking**
   - Meta-learner concept
   - Level 0 and Level 1 models
   - Cross-validation in stacking
   - Blending vs stacking

5. **Diversity in Ensembles**
   - Data diversity (different samples)
   - Feature diversity (different features)
   - Algorithm diversity (different models)
   - Parameter diversity

6. **Hyperparameter Tuning**
   - Tuning ensemble parameters
   - Learning rate in boosting
   - Number of estimators
   - Regularization in boosting

7. **Out-of-Fold Predictions**
   - Using OOF predictions as features
   - Multi-level stacking
   - Avoiding data leakage

---

## Learning Resources

- **Libraries:** Scikit-learn, XGBoost, LightGBM, CatBoost
- **Datasets:** Classification and regression benchmarks
- **Tools:** GridSearchCV, ensemble voting, stacking utilities

---

## Assessment Criteria

- Correct implementation of ensemble methods
- Appropriate algorithm selection
- Performance improvement demonstration
- Ensemble diversity understanding
- Hyperparameter optimization
- Code documentation

---

## Next Steps

After completing this practical:
- Move to Practical 11: Advanced optimization techniques
- Work on Kaggle competitions with ensembles
- Explore neural network ensembles
