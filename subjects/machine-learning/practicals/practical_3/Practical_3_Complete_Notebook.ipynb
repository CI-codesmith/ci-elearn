{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa4428e",
   "metadata": {},
   "source": [
    "## Student Information\n",
    "\n",
    "**Name:** [Your Name Here]  \n",
    "**Roll Number:** [Your Roll Number]  \n",
    "**Date:** [Submission Date]  \n",
    "**College:** [Your College Name]  \n",
    "**Academic Year:** 2025-2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Learning Outcomes Checklist\n",
    "\n",
    "- [ ] LO1: Read CSV files with various parameters and encodings\n",
    "- [ ] LO2: Parse JSON data including nested structures\n",
    "- [ ] LO3: Extract data from XML files using multiple approaches\n",
    "- [ ] LO4: Read Excel spreadsheets and multiple sheets\n",
    "- [ ] LO5: Connect and query SQL databases\n",
    "- [ ] LO6: Handle file encoding issues and special characters\n",
    "- [ ] LO7: Validate loaded data for quality and consistency\n",
    "- [ ] LO8: Convert between different file formats\n",
    "- [ ] LO9: Optimize data loading performance\n",
    "- [ ] LO10: Create reusable data loading functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe0e10f",
   "metadata": {},
   "source": [
    "## Phase 0: Environment Setup & Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e98d3be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö LIBRARY VERSIONS:\n",
      "Python: 3.13.5\n",
      "Pandas: 2.2.2\n",
      "NumPy: 1.26.4\n",
      "\n",
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import sys\n",
    "import sqlite3\n",
    "\n",
    "# Print versions\n",
    "print(\"üìö LIBRARY VERSIONS:\")\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "\n",
    "# Set up display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52a7a9",
   "metadata": {},
   "source": [
    "## Phase 1: CSV File Reading\n",
    "\n",
    "CSV (Comma-Separated Values) is the most common format for data exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc712888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample CSV file created!\n",
      "\n",
      "First few lines of CSV:\n",
      "ID,Name,Age,Department,Salary\n",
      "1,Alice,28,Engineering,75000\n",
      "2,Bob,34,Sales,65000\n",
      "3,Charlie,29,Marketing,60000\n",
      "4,Diana,31,Engineering,80000\n",
      "5,Eve,26,HR,55000\n",
      "6,Frank,35,Finance,70000\n",
      "7,Grace,27,Engineer...\n"
     ]
    }
   ],
   "source": [
    "# Create sample CSV data for demonstration\n",
    "sample_csv_data = '''ID,Name,Age,Department,Salary\n",
    "1,Alice,28,Engineering,75000\n",
    "2,Bob,34,Sales,65000\n",
    "3,Charlie,29,Marketing,60000\n",
    "4,Diana,31,Engineering,80000\n",
    "5,Eve,26,HR,55000\n",
    "6,Frank,35,Finance,70000\n",
    "7,Grace,27,Engineering,72000\n",
    "8,Henry,33,Sales,68000\n",
    "9,Iris,30,Marketing,62000\n",
    "10,Jack,36,Finance,75000'''\n",
    "\n",
    "# Write to CSV file\n",
    "with open('sample_data.csv', 'w') as f:\n",
    "    f.write(sample_csv_data)\n",
    "\n",
    "print(\"‚úÖ Sample CSV file created!\")\n",
    "print(\"\\nFirst few lines of CSV:\")\n",
    "print(sample_csv_data[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9789dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ BASIC CSV READING:\n",
      "Shape: (10, 5)\n",
      "\n",
      "First 5 rows:\n",
      "   ID     Name  Age   Department  Salary\n",
      "0   1    Alice   28  Engineering   75000\n",
      "1   2      Bob   34        Sales   65000\n",
      "2   3  Charlie   29    Marketing   60000\n",
      "3   4    Diana   31  Engineering   80000\n",
      "4   5      Eve   26           HR   55000\n",
      "\n",
      "Data Types:\n",
      "ID             int64\n",
      "Name          object\n",
      "Age            int64\n",
      "Department    object\n",
      "Salary         int64\n",
      "dtype: object\n",
      "\n",
      "Memory Usage: 1.44 KB\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV reading\n",
    "print(\"üìñ BASIC CSV READING:\")\n",
    "df_csv = pd.read_csv('sample_data.csv')\n",
    "print(f\"Shape: {df_csv.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(df_csv.head())\n",
    "print(f\"\\nData Types:\")\n",
    "print(df_csv.dtypes)\n",
    "print(f\"\\nMemory Usage: {df_csv.memory_usage(deep=True).sum() / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3783eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ CSV WITH CUSTOM PARAMETERS:\n",
      "‚úÖ Custom CSV loaded: (5, 5)\n",
      "   ID     Name  Age   Department  Salary\n",
      "0   1    Alice   28  Engineering   75000\n",
      "1   2      Bob   34        Sales   65000\n",
      "2   3  Charlie   29    Marketing   60000\n",
      "3   4    Diana   31  Engineering   80000\n",
      "4   5      Eve   26           HR   55000\n"
     ]
    }
   ],
   "source": [
    "# Read CSV with custom parameters\n",
    "print(\"üìñ CSV WITH CUSTOM PARAMETERS:\")\n",
    "\n",
    "df_custom = pd.read_csv(\n",
    "    'sample_data.csv',\n",
    "    delimiter=',',                    # CSV delimiter\n",
    "    header=0,                         # Row index for column names\n",
    "    encoding='utf-8',                 # File encoding\n",
    "    dtype={'ID': int, 'Name': str},   # Specify column types\n",
    "    nrows=5                           # Read first 5 rows\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Custom CSV loaded: {df_custom.shape}\")\n",
    "print(df_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a88e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ENCODING HANDLING:\n",
      "‚úÖ Successfully loaded with encoding: utf-8\n",
      "Loaded shape: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "# Handle encoding issues\n",
    "print(\"üìñ ENCODING HANDLING:\")\n",
    "\n",
    "def read_csv_with_encoding(filepath):\n",
    "    \"\"\"Try different encodings if UTF-8 fails\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']\n",
    "    \n",
    "    for enc in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, encoding=enc)\n",
    "            print(f\"‚úÖ Successfully loaded with encoding: {enc}\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    print(\"‚ùå Could not read file with any encoding\")\n",
    "    return None\n",
    "\n",
    "# Test with our sample file\n",
    "df_encoded = read_csv_with_encoding('sample_data.csv')\n",
    "print(f\"Loaded shape: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c8376",
   "metadata": {},
   "source": [
    "## Phase 2: JSON File Reading\n",
    "\n",
    "JSON (JavaScript Object Notation) is great for hierarchical and nested data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709e4c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample JSON file created!\n",
      "\n",
      "JSON content preview:\n",
      "[\n",
      "  {\n",
      "    \"ID\": 1,\n",
      "    \"Name\": \"Alice\",\n",
      "    \"Age\": 28,\n",
      "    \"Department\": \"Engineering\",\n",
      "    \"Salary\": 75000\n",
      "  },\n",
      "  {\n",
      "    \"ID\": 2,\n",
      "    \"Name\": \"Bob\",\n",
      "    \"Age\": 34,\n",
      "    \"Department\": \"Sales\",\n",
      "    \"Salary\": 65000\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Create sample JSON data\n",
    "sample_json_data = [\n",
    "    {\"ID\": 1, \"Name\": \"Alice\", \"Age\": 28, \"Department\": \"Engineering\", \"Salary\": 75000},\n",
    "    {\"ID\": 2, \"Name\": \"Bob\", \"Age\": 34, \"Department\": \"Sales\", \"Salary\": 65000},\n",
    "    {\"ID\": 3, \"Name\": \"Charlie\", \"Age\": 29, \"Department\": \"Marketing\", \"Salary\": 60000},\n",
    "    {\"ID\": 4, \"Name\": \"Diana\", \"Age\": 31, \"Department\": \"Engineering\", \"Salary\": 80000},\n",
    "    {\"ID\": 5, \"Name\": \"Eve\", \"Age\": 26, \"Department\": \"HR\", \"Salary\": 55000}\n",
    "]\n",
    "\n",
    "# Write to JSON file\n",
    "with open('sample_data.json', 'w') as f:\n",
    "    json.dump(sample_json_data, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Sample JSON file created!\")\n",
    "print(f\"\\nJSON content preview:\")\n",
    "print(json.dumps(sample_json_data[:2], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fcce924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ BASIC JSON READING:\n",
      "Shape: (5, 5)\n",
      "\n",
      "Loaded data:\n",
      "   ID     Name  Age   Department  Salary\n",
      "0   1    Alice   28  Engineering   75000\n",
      "1   2      Bob   34        Sales   65000\n",
      "2   3  Charlie   29    Marketing   60000\n",
      "3   4    Diana   31  Engineering   80000\n",
      "4   5      Eve   26           HR   55000\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file\n",
    "print(\"üìñ BASIC JSON READING:\")\n",
    "df_json = pd.read_json('sample_data.json')\n",
    "print(f\"Shape: {df_json.shape}\")\n",
    "print(f\"\\nLoaded data:\")\n",
    "print(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5c76ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ NESTED JSON HANDLING:\n",
      "Nested JSON normalized:\n",
      "   id   name  details.age details.dept\n",
      "0   1  Alice           28          Eng\n",
      "1   2    Bob           34        Sales\n",
      "\n",
      "Columns: ['id', 'name', 'details.age', 'details.dept']\n"
     ]
    }
   ],
   "source": [
    "# Handle nested JSON\n",
    "print(\"üìñ NESTED JSON HANDLING:\")\n",
    "\n",
    "nested_json_data = {\n",
    "    \"employees\": [\n",
    "        {\"id\": 1, \"name\": \"Alice\", \"details\": {\"age\": 28, \"dept\": \"Eng\"}},\n",
    "        {\"id\": 2, \"name\": \"Bob\", \"details\": {\"age\": 34, \"dept\": \"Sales\"}}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('nested_data.json', 'w') as f:\n",
    "    json.dump(nested_json_data, f, indent=2)\n",
    "\n",
    "# Read and normalize nested JSON\n",
    "with open('nested_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_nested = pd.json_normalize(data['employees'])\n",
    "print(f\"Nested JSON normalized:\")\n",
    "print(df_nested)\n",
    "print(f\"\\nColumns: {list(df_nested.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed88dfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ JSON STRING PARSING:\n",
      "Parsed from string: (3, 3)\n",
      "   id     name     city\n",
      "0   1    Alice      NYC\n",
      "1   2      Bob       LA\n",
      "2   3  Charlie  Chicago\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/fbr9l6wn4c570d0d4d7f_d880000gn/T/ipykernel_5510/770485135.py:10: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df_from_string = pd.read_json(json_string, orient='records')\n"
     ]
    }
   ],
   "source": [
    "# JSON from string\n",
    "print(\"üìñ JSON STRING PARSING:\")\n",
    "\n",
    "json_string = '''[\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"city\": \"NYC\"},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"city\": \"LA\"},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"city\": \"Chicago\"}\n",
    "]'''\n",
    "\n",
    "df_from_string = pd.read_json(json_string, orient='records')\n",
    "print(f\"Parsed from string: {df_from_string.shape}\")\n",
    "print(df_from_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74faf079",
   "metadata": {},
   "source": [
    "## Phase 3: XML File Reading\n",
    "\n",
    "XML (eXtensible Markup Language) is a hierarchical format often used in enterprise systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c01ada79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample XML file created!\n",
      "\n",
      "XML structure:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<employees>\n",
      "    <employee>\n",
      "        <id>1</id>\n",
      "        <name>Alice</name>\n",
      "        <age>28</age>\n",
      "        <department>Engineering</department>\n",
      "        <salary>75000</salary>\n",
      "    </employee>\n",
      "    <employee>\n",
      "        <id>2</id>\n",
      "        <name>Bob</name>\n",
      "        <age>34...\n"
     ]
    }
   ],
   "source": [
    "# Create sample XML data\n",
    "xml_data = '''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<employees>\n",
    "    <employee>\n",
    "        <id>1</id>\n",
    "        <name>Alice</name>\n",
    "        <age>28</age>\n",
    "        <department>Engineering</department>\n",
    "        <salary>75000</salary>\n",
    "    </employee>\n",
    "    <employee>\n",
    "        <id>2</id>\n",
    "        <name>Bob</name>\n",
    "        <age>34</age>\n",
    "        <department>Sales</department>\n",
    "        <salary>65000</salary>\n",
    "    </employee>\n",
    "    <employee>\n",
    "        <id>3</id>\n",
    "        <name>Charlie</name>\n",
    "        <age>29</age>\n",
    "        <department>Marketing</department>\n",
    "        <salary>60000</salary>\n",
    "    </employee>\n",
    "</employees>'''\n",
    "\n",
    "# Write to XML file\n",
    "with open('sample_data.xml', 'w') as f:\n",
    "    f.write(xml_data)\n",
    "\n",
    "print(\"‚úÖ Sample XML file created!\")\n",
    "print(f\"\\nXML structure:\")\n",
    "print(xml_data[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2d0c4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ XML PARSING WITH ELEMENTTREE:\n",
      "Root tag: employees\n",
      "Number of records: 3\n",
      "\n",
      "‚úÖ XML parsed successfully: (3, 5)\n",
      "  id     name age   department salary\n",
      "0  1    Alice  28  Engineering  75000\n",
      "1  2      Bob  34        Sales  65000\n",
      "2  3  Charlie  29    Marketing  60000\n"
     ]
    }
   ],
   "source": [
    "# Parse XML using ElementTree\n",
    "print(\"üìñ XML PARSING WITH ELEMENTTREE:\")\n",
    "\n",
    "tree = ET.parse('sample_data.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(f\"Root tag: {root.tag}\")\n",
    "print(f\"Number of records: {len(root)}\")\n",
    "\n",
    "# Extract data into list\n",
    "data_list = []\n",
    "for item in root.findall('employee'):\n",
    "    record = {\n",
    "        'id': item.find('id').text,\n",
    "        'name': item.find('name').text,\n",
    "        'age': item.find('age').text,\n",
    "        'department': item.find('department').text,\n",
    "        'salary': item.find('salary').text\n",
    "    }\n",
    "    data_list.append(record)\n",
    "\n",
    "df_xml = pd.DataFrame(data_list)\n",
    "print(f\"\\n‚úÖ XML parsed successfully: {df_xml.shape}\")\n",
    "print(df_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b456c85",
   "metadata": {},
   "source": [
    "## Phase 4: Excel File Reading\n",
    "\n",
    "Excel is widely used in business environments, supporting multiple sheets and formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a4779b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù CREATING SAMPLE EXCEL FILE:\n",
      "‚úÖ Excel file with 2 sheets created!\n"
     ]
    }
   ],
   "source": [
    "# Create sample Excel file\n",
    "print(\"üìù CREATING SAMPLE EXCEL FILE:\")\n",
    "\n",
    "# Create multiple dataframes\n",
    "df_sheet1 = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Department': ['Engineering', 'Sales', 'Marketing', 'Engineering', 'HR']\n",
    "})\n",
    "\n",
    "df_sheet2 = pd.DataFrame({\n",
    "    'Department': ['Engineering', 'Sales', 'Marketing', 'HR'],\n",
    "    'Budget': [500000, 300000, 200000, 150000]\n",
    "})\n",
    "\n",
    "# Write to Excel with multiple sheets\n",
    "with pd.ExcelWriter('sample_data.xlsx', engine='openpyxl') as writer:\n",
    "    df_sheet1.to_excel(writer, sheet_name='Employees', index=False)\n",
    "    df_sheet2.to_excel(writer, sheet_name='Budgets', index=False)\n",
    "\n",
    "print(\"‚úÖ Excel file with 2 sheets created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "504c9bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ BASIC EXCEL READING:\n",
      "Shape: (5, 3)\n",
      "\n",
      "First sheet data:\n",
      "   ID     Name   Department\n",
      "0   1    Alice  Engineering\n",
      "1   2      Bob        Sales\n",
      "2   3  Charlie    Marketing\n",
      "3   4    Diana  Engineering\n",
      "4   5      Eve           HR\n"
     ]
    }
   ],
   "source": [
    "# Read Excel file\n",
    "print(\"üìñ BASIC EXCEL READING:\")\n",
    "\n",
    "df_excel = pd.read_excel('sample_data.xlsx')\n",
    "print(f\"Shape: {df_excel.shape}\")\n",
    "print(f\"\\nFirst sheet data:\")\n",
    "print(df_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0b1007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ READING SPECIFIC SHEETS:\n",
      "Available sheets: ['Employees', 'Budgets']\n",
      "\n",
      "Employees sheet: (5, 3)\n",
      "   ID     Name   Department\n",
      "0   1    Alice  Engineering\n",
      "1   2      Bob        Sales\n",
      "2   3  Charlie    Marketing\n",
      "3   4    Diana  Engineering\n",
      "4   5      Eve           HR\n",
      "\n",
      "Budgets sheet: (4, 2)\n",
      "    Department  Budget\n",
      "0  Engineering  500000\n",
      "1        Sales  300000\n",
      "2    Marketing  200000\n",
      "3           HR  150000\n"
     ]
    }
   ],
   "source": [
    "# Read specific sheets\n",
    "print(\"üìñ READING SPECIFIC SHEETS:\")\n",
    "\n",
    "# Get all sheet names\n",
    "xl_file = pd.ExcelFile('sample_data.xlsx')\n",
    "print(f\"Available sheets: {xl_file.sheet_names}\")\n",
    "\n",
    "# Read specific sheet\n",
    "df_employees = pd.read_excel('sample_data.xlsx', sheet_name='Employees')\n",
    "df_budgets = pd.read_excel('sample_data.xlsx', sheet_name='Budgets')\n",
    "\n",
    "print(f\"\\nEmployees sheet: {df_employees.shape}\")\n",
    "print(df_employees)\n",
    "\n",
    "print(f\"\\nBudgets sheet: {df_budgets.shape}\")\n",
    "print(df_budgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c0443b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ READING ALL SHEETS:\n",
      "‚úÖ 2 sheets loaded\n",
      "\n",
      "Sheet: Employees - Shape: (5, 3)\n",
      "Sheet: Budgets - Shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "# Read all sheets at once\n",
    "print(\"üìñ READING ALL SHEETS:\")\n",
    "\n",
    "all_sheets = pd.read_excel('sample_data.xlsx', sheet_name=None)\n",
    "print(f\"‚úÖ {len(all_sheets)} sheets loaded\\n\")\n",
    "\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    print(f\"Sheet: {sheet_name} - Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f9b705",
   "metadata": {},
   "source": [
    "## Phase 5: SQL Database Reading\n",
    "\n",
    "SQL databases provide structured data storage with querying capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93278984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÑÔ∏è CREATING SAMPLE DATABASE:\n",
      "‚úÖ SQLite database created with sample data!\n",
      "üìñ READING FROM SQLITE (METHOD 1 - sqlite3):\n",
      "Shape: (5, 5)\n",
      "   id     name  age   department   salary\n",
      "0   1    Alice   28  Engineering  75000.0\n",
      "1   2      Bob   34        Sales  65000.0\n",
      "2   3  Charlie   29    Marketing  60000.0\n",
      "3   4    Diana   31  Engineering  80000.0\n",
      "4   5      Eve   26           HR  55000.0\n"
     ]
    }
   ],
   "source": [
    "# Create sample SQLite database\n",
    "print(\"üóÑÔ∏è CREATING SAMPLE DATABASE:\")\n",
    "\n",
    "try:\n",
    "    # Create connection\n",
    "    with sqlite3.connect('sample_database.db') as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create table\n",
    "        cursor.execute('''CREATE TABLE IF NOT EXISTS employees (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            name TEXT,\n",
    "            age INTEGER,\n",
    "            department TEXT,\n",
    "            salary REAL\n",
    "        )''')\n",
    "\n",
    "        # Insert sample data\n",
    "        data = [\n",
    "            (1, 'Alice', 28, 'Engineering', 75000),\n",
    "            (2, 'Bob', 34, 'Sales', 65000),\n",
    "            (3, 'Charlie', 29, 'Marketing', 60000),\n",
    "            (4, 'Diana', 31, 'Engineering', 80000),\n",
    "            (5, 'Eve', 26, 'HR', 55000)\n",
    "        ]\n",
    "\n",
    "        cursor.executemany('INSERT OR IGNORE INTO employees VALUES (?, ?, ?, ?, ?)', data)\n",
    "        conn.commit()\n",
    "\n",
    "    print(\"‚úÖ SQLite database created with sample data!\")\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"‚ùå SQLite error: {e}\")\n",
    "\n",
    "# Read from SQLite using sqlite3\n",
    "print(\"üìñ READING FROM SQLITE (METHOD 1 - sqlite3):\")\n",
    "\n",
    "try:\n",
    "    with sqlite3.connect('sample_database.db') as conn:\n",
    "        df_sql_sqlite = pd.read_sql_query('SELECT * FROM employees', conn)\n",
    "\n",
    "    print(f\"Shape: {df_sql_sqlite.shape}\")\n",
    "    print(df_sql_sqlite)\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"‚ùå SQLite error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb439412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ READING FROM SQLITE (METHOD 2 - SQLAlchemy):\n",
      "‚ùå NameError: name 'pd' is not defined. Ensure all required libraries are imported.\n"
     ]
    }
   ],
   "source": [
    "# Read using SQLAlchemy (more powerful)\n",
    "print(\"üìñ READING FROM SQLITE (METHOD 2 - SQLAlchemy):\")\n",
    "\n",
    "try:\n",
    "    from sqlalchemy import create_engine\n",
    "\n",
    "    # Create engine\n",
    "    engine = create_engine('sqlite:///sample_database.db')\n",
    "\n",
    "    # Method 1: Read entire table\n",
    "    df_sql_table = pd.read_sql_table('employees', engine)\n",
    "    print(f\"Table read: {df_sql_table.shape}\")\n",
    "\n",
    "    # Method 2: Read from SQL query\n",
    "    df_sql_query = pd.read_sql_query('SELECT * FROM employees WHERE salary > 60000', engine)\n",
    "    print(f\"Query result (salary > 60000): {df_sql_query.shape}\")\n",
    "    print(df_sql_query)\n",
    "\n",
    "    # Dispose of engine\n",
    "    engine.dispose()\n",
    "\n",
    "except ImportError:\n",
    "    print(\"‚ùå SQLAlchemy is not installed. Please install it to use this method.\")\n",
    "except NameError as e:\n",
    "    print(f\"‚ùå NameError: {e}. Ensure all required libraries are imported.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading from SQLite using SQLAlchemy: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3443331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ ADVANCED SQL QUERIES:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìñ ADVANCED SQL QUERIES:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Query 1: Aggregate functions\u001b[39;00m\n\u001b[32m      5\u001b[39m df_agg = pd.read_sql_query(\u001b[33m'''\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m    SELECT department, COUNT(*) as count, AVG(salary) as avg_salary\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33m    FROM employees\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33m    GROUP BY department\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[33m'''\u001b[39m, \u001b[43mengine\u001b[49m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDepartment statistics:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdf_agg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Query 2: Sorting and limiting\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "# Advanced SQL queries\n",
    "print(\"üìñ ADVANCED SQL QUERIES:\")\n",
    "\n",
    "# Query 1: Aggregate functions\n",
    "df_agg = pd.read_sql_query('''\n",
    "    SELECT department, COUNT(*) as count, AVG(salary) as avg_salary\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "''', engine)\n",
    "\n",
    "print(f\"Department statistics:\\n{df_agg}\\n\")\n",
    "\n",
    "# Query 2: Sorting and limiting\n",
    "df_sorted = pd.read_sql_query('''\n",
    "    SELECT name, department, salary\n",
    "    FROM employees\n",
    "    ORDER BY salary DESC\n",
    "    LIMIT 3\n",
    "''', engine)\n",
    "\n",
    "print(f\"Top 3 by salary:\\n{df_sorted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e899055",
   "metadata": {},
   "source": [
    "## Phase 6: Data Validation & Comparison\n",
    "\n",
    "After loading data, it's crucial to validate and compare different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate_dataframe(df, name):\n",
    "    \"\"\"Validate and report on DataFrame quality\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VALIDATION REPORT: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory Usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "    print(f\"Missing Values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "    print(f\"Duplicate Columns: {len(df.columns) - len(set(df.columns))}\")\n",
    "    print(f\"Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "# Validate all loaded files\n",
    "validate_dataframe(df_csv, \"CSV File\")\n",
    "validate_dataframe(df_json, \"JSON File\")\n",
    "validate_dataframe(df_xml, \"XML File\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different formats\n",
    "print(\"\\nüìä FORMAT COMPARISON:\\n\")\n",
    "\n",
    "formats = {\n",
    "    'CSV': df_csv,\n",
    "    'JSON': df_json,\n",
    "    'XML': df_xml,\n",
    "    'Excel': df_employees,\n",
    "    'SQL': df_sql_table\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_data = []\n",
    "for name, df in formats.items():\n",
    "    if df is not None:\n",
    "        comparison_data.append({\n",
    "            'Format': name,\n",
    "            'Rows': df.shape[0],\n",
    "            'Columns': df.shape[1],\n",
    "            'Memory (KB)': f\"{df.memory_usage(deep=True).sum() / 1024:.2f}\",\n",
    "            'Data Types': len(df.dtypes.unique())\n",
    "        })\n",
    "    else:\n",
    "        print(f\"‚ùå Missing data for format: {name}\")\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3a934",
   "metadata": {},
   "source": [
    "## Phase 7: Universal Data Loader Function\n",
    "\n",
    "Create a reusable function that automatically detects and loads different file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382bb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, **kwargs):\n",
    "    \"\"\"\n",
    "    Automatically detect file format and load data\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: Path to file\n",
    "    - **kwargs: Additional parameters for specific format\n",
    "    \n",
    "    Returns:\n",
    "    - pandas DataFrame or None if error\n",
    "    \n",
    "    Example:\n",
    "    df = load_dataset('data.csv')\n",
    "    df = load_dataset('data.json', orient='records')\n",
    "    df = load_dataset('data.xlsx', sheet_name='Sheet1')\n",
    "    \"\"\"\n",
    "    extension = file_path.split('.')[-1].lower()\n",
    "    \n",
    "    try:\n",
    "        if extension == 'csv':\n",
    "            df = pd.read_csv(file_path, **kwargs)\n",
    "            print(f\"‚úÖ CSV file loaded: {df.shape}\")\n",
    "            return df\n",
    "        \n",
    "        elif extension == 'json':\n",
    "            df = pd.read_json(file_path, **kwargs)\n",
    "            print(f\"‚úÖ JSON file loaded: {df.shape}\")\n",
    "            return df\n",
    "        \n",
    "        elif extension in ['xlsx', 'xls']:\n",
    "            df = pd.read_excel(file_path, **kwargs)\n",
    "            print(f\"‚úÖ Excel file loaded: {df.shape}\")\n",
    "            return df\n",
    "        \n",
    "        elif extension == 'xml':\n",
    "            try:\n",
    "                tree = ET.parse(file_path)\n",
    "                root = tree.getroot()\n",
    "                print(f\"‚úÖ XML file parsed (root tag: {root.tag})\")\n",
    "                return root\n",
    "            except ET.ParseError as e:\n",
    "                print(f\"‚ùå XML parsing error: {e}\")\n",
    "                return None\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {extension}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Universal loader function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test universal loader\n",
    "print(\"üß™ TESTING UNIVERSAL LOADER:\\n\")\n",
    "\n",
    "df_auto_csv = load_dataset('sample_data.csv')\n",
    "df_auto_json = load_dataset('sample_data.json')\n",
    "df_auto_excel = load_dataset('sample_data.xlsx')\n",
    "\n",
    "print(\"\\n‚úÖ All formats loaded successfully with universal loader!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d84b74",
   "metadata": {},
   "source": [
    "## Phase 8: Performance Optimization\n",
    "\n",
    "Learn techniques to optimize data loading for large files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4afe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison\n",
    "print(\"‚ö° PERFORMANCE TESTING:\\n\")\n",
    "\n",
    "# Create larger test files\n",
    "np.random.seed(42)\n",
    "large_df = pd.DataFrame({\n",
    "    'id': range(10000),\n",
    "    'value1': np.random.randint(0, 100, 10000),\n",
    "    'value2': np.random.randn(10000),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 10000)\n",
    "})\n",
    "\n",
    "# Save in different formats\n",
    "large_df.to_csv('large_data.csv', index=False)\n",
    "large_df.to_json('large_data.json', orient='records')\n",
    "large_df.to_excel('large_data.xlsx', index=False)\n",
    "\n",
    "print(\"Large test files created (10,000 rows, 4 columns)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fb2575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time different loading methods\n",
    "print(\"üìä LOAD TIME COMPARISON:\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "# CSV loading\n",
    "start = time.time()\n",
    "df_time_csv = pd.read_csv('large_data.csv')\n",
    "csv_time = time.time() - start\n",
    "results.append({'Format': 'CSV', 'Time (s)': f\"{csv_time:.4f}\"})\n",
    "\n",
    "# JSON loading\n",
    "start = time.time()\n",
    "df_time_json = pd.read_json('large_data.json', orient='records')\n",
    "json_time = time.time() - start\n",
    "results.append({'Format': 'JSON', 'Time (s)': f\"{json_time:.4f}\"})\n",
    "\n",
    "# Excel loading\n",
    "start = time.time()\n",
    "df_time_excel = pd.read_excel('large_data.xlsx')\n",
    "excel_time = time.time() - start\n",
    "results.append({'Format': 'Excel', 'Time (s)': f\"{excel_time:.4f}\"})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nüí° Fastest: CSV ({csv_time:.4f}s)\")\n",
    "print(f\"   Slowest: Excel ({excel_time:.4f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ded2aec",
   "metadata": {},
   "source": [
    "## Phase 9: Practical Tests\n",
    "\n",
    "Complete all 5 tests to verify your learning outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: CSV LOADING\n",
    "print(\"üß™ TEST 1: CSV LOADING\")\n",
    "try:\n",
    "    assert df_csv.shape[0] > 0, \"CSV has no rows!\"\n",
    "    assert df_csv.shape[1] > 0, \"CSV has no columns!\"\n",
    "    assert 'ID' in df_csv.columns, \"ID column missing!\"\n",
    "    assert 'Name' in df_csv.columns, \"Name column missing!\"\n",
    "    assert 'Age' in df_csv.columns, \"Age column missing!\"\n",
    "    print(\"‚úÖ TEST 1 PASSED: CSV Loading\")\n",
    "    test1_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå TEST 1 FAILED: {e}\")\n",
    "    test1_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f19484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: JSON PARSING\n",
    "print(\"\\nüß™ TEST 2: JSON PARSING\")\n",
    "try:\n",
    "    assert df_json.shape[0] > 0, \"JSON has no rows!\"\n",
    "    assert isinstance(df_json, pd.DataFrame), \"JSON not converted to DataFrame!\"\n",
    "    assert len(df_json.columns) > 0, \"JSON has no columns!\"\n",
    "    print(f\"‚úÖ TEST 2 PASSED: JSON Parsing ({df_json.shape[0]} records)\")\n",
    "    test2_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå TEST 2 FAILED: {e}\")\n",
    "    test2_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad98e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: XML EXTRACTION\n",
    "print(\"\\nüß™ TEST 3: XML EXTRACTION\")\n",
    "try:\n",
    "    assert df_xml.shape[0] > 0, \"XML has no rows!\"\n",
    "    assert 'id' in df_xml.columns or 'name' in df_xml.columns, \"Required columns missing from XML!\"\n",
    "    assert isinstance(df_xml, pd.DataFrame), \"XML not converted to DataFrame!\"\n",
    "    print(f\"‚úÖ TEST 3 PASSED: XML Extraction ({df_xml.shape[0]} records)\")\n",
    "    test3_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå TEST 3 FAILED: {e}\")\n",
    "    test3_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 4: EXCEL READING\n",
    "print(\"\\nüß™ TEST 4: EXCEL READING\")\n",
    "try:\n",
    "    assert df_excel.shape[0] > 0, \"Excel has no rows!\"\n",
    "    assert df_excel.shape[1] > 0, \"Excel has no columns!\"\n",
    "    xl_file = pd.ExcelFile('sample_data.xlsx')\n",
    "    assert len(xl_file.sheet_names) >= 1, \"No sheets found!\"\n",
    "    print(f\"‚úÖ TEST 4 PASSED: Excel Reading ({len(xl_file.sheet_names)} sheets found)\")\n",
    "    test4_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå TEST 4 FAILED: {e}\")\n",
    "    test4_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb28a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 5: SQL QUERY\n",
    "print(\"\\nüß™ TEST 5: SQL QUERY\")\n",
    "try:\n",
    "    assert df_sql_table.shape[0] > 0, \"SQL query returned no rows!\"\n",
    "    assert df_sql_table.shape[1] > 0, \"SQL query returned no columns!\"\n",
    "    assert 'name' in df_sql_table.columns or 'Name' in df_sql_table.columns, \"Name column missing from SQL!\"\n",
    "    print(f\"‚úÖ TEST 5 PASSED: SQL Query ({df_sql_table.shape[0]} records retrieved)\")\n",
    "    test5_result = \"PASSED\"\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå TEST 5 FAILED: {e}\")\n",
    "    test5_result = \"FAILED\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e27c0b",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "### Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18145f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "test_summary = pd.DataFrame([\n",
    "    {'Test': 'Test 1: CSV Loading', 'Result': test1_result},\n",
    "    {'Test': 'Test 2: JSON Parsing', 'Result': test2_result},\n",
    "    {'Test': 'Test 3: XML Extraction', 'Result': test3_result},\n",
    "    {'Test': 'Test 4: Excel Reading', 'Result': test4_result},\n",
    "    {'Test': 'Test 5: SQL Query', 'Result': test5_result}\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(test_summary.to_string(index=False))\n",
    "\n",
    "passed = sum([1 for r in [test1_result, test2_result, test3_result, test4_result, test5_result] if r == 'PASSED'])\n",
    "total = 5\n",
    "print(f\"\\nüìä SCORE: {passed}/{total} TESTS PASSED ({passed*100/total:.0f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b299154f",
   "metadata": {},
   "source": [
    "## Reflections & Learnings\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "1. **CSV Files:**\n",
    "   - Most efficient format for tabular data\n",
    "   - Easy to read and process\n",
    "   - Handling encoding issues important for international data\n",
    "\n",
    "2. **JSON Format:**\n",
    "   - Excellent for hierarchical/nested data\n",
    "   - Use `json_normalize()` for flattening nested structures\n",
    "   - Good for APIs and web services\n",
    "\n",
    "3. **XML Format:**\n",
    "   - Complex parsing required\n",
    "   - Verbose format but well-structured\n",
    "   - Common in enterprise systems\n",
    "\n",
    "4. **Excel Files:**\n",
    "   - Support multiple sheets\n",
    "   - Requires additional libraries (openpyxl)\n",
    "   - Useful for business reports\n",
    "\n",
    "5. **SQL Databases:**\n",
    "   - Best for large, structured data\n",
    "   - Efficient querying with WHERE, GROUP BY, etc.\n",
    "   - Scalable for production systems\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Always validate data after loading\n",
    "- Choose format based on data characteristics\n",
    "- Handle encoding issues proactively\n",
    "- Create reusable functions for efficiency\n",
    "- Test different approaches for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ce5cc",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before submitting your practical, verify:\n",
    "\n",
    "### Code Completion\n",
    "- [ ] Phase 1 (CSV): Completed with encoding handling\n",
    "- [ ] Phase 2 (JSON): Completed with nested data handling\n",
    "- [ ] Phase 3 (XML): Completed with ElementTree parsing\n",
    "- [ ] Phase 4 (Excel): Completed with multiple sheet reading\n",
    "- [ ] Phase 5 (SQL): Completed with queries\n",
    "- [ ] Phase 6 (Validation): Data validation completed\n",
    "- [ ] Phase 7 (Loader): Universal function created\n",
    "- [ ] Phase 8 (Performance): Optimization tested\n",
    "\n",
    "### Test Results\n",
    "- [ ] All 5 tests passed ‚úÖ\n",
    "- [ ] Test output clearly visible\n",
    "- [ ] Error handling demonstrated\n",
    "- [ ] Performance metrics recorded\n",
    "\n",
    "### Documentation\n",
    "- [ ] Student information filled\n",
    "- [ ] Learning outcomes checklist marked\n",
    "- [ ] Reflections written\n",
    "- [ ] Code is well-commented\n",
    "- [ ] All output is visible and clear\n",
    "\n",
    "### Files\n",
    "- [ ] Notebook saved as `Practical_3_Complete_Notebook.ipynb`\n",
    "- [ ] PDF exported from notebook\n",
    "- [ ] All test data files included\n",
    "- [ ] No errors in notebook execution\n",
    "\n",
    "---\n",
    "\n",
    "**Practical Status:** Ready for submission ‚úÖ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
