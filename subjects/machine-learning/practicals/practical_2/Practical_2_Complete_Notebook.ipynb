{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d137c61",
   "metadata": {},
   "source": [
    "## üìå Student Information\n",
    "\n",
    "**Student Name:** _____________________________  \n",
    "**Roll Number:** ____________________________  \n",
    "**Date of Practical:** ______________________  \n",
    "**IDE Used:** ‚òê Jupyter ‚òê Anaconda ‚òê VS Code ‚òê Colab ‚òê PyCharm  \n",
    "**Practical Status:** ‚òê In Progress ‚òê Completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf7ce5",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "This cell imports all necessary libraries for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e3e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Scikit-learn Version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa07a3",
   "metadata": {},
   "source": [
    "## Step 2: Create Sample Dataset\n",
    "\n",
    "Since we don't have an external file, we'll create a realistic sample dataset with common data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51552f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create sample dataset with realistic data quality issues\n",
    "n_samples = 300\n",
    "\n",
    "data = {\n",
    "    'price': np.random.normal(300000, 100000, n_samples),\n",
    "    'square_feet': np.random.normal(2000, 500, n_samples),\n",
    "    'bedrooms': np.random.choice([1, 2, 3, 4, 5, np.nan], n_samples),\n",
    "    'bathrooms': np.random.choice([1, 1.5, 2, 2.5, 3, np.nan], n_samples),\n",
    "    'age': np.random.choice(range(0, 100), n_samples),\n",
    "    'city': np.random.choice(['New York', 'new york', 'Los Angeles', 'los angeles', 'Chicago', np.nan], n_samples),\n",
    "    'garage': np.random.choice([0, 1, 2, 3, np.nan], n_samples)\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(data)\n",
    "\n",
    "# Add some outliers intentionally\n",
    "df_raw.loc[10, 'price'] = 5000000  # Unrealistic price\n",
    "df_raw.loc[20, 'square_feet'] = 10000  # Unrealistic size\n",
    "\n",
    "print(\"‚úÖ Sample dataset created!\")\n",
    "print(f\"\\nDataset Shape: {df_raw.shape}\")\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(df_raw.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99234ab2",
   "metadata": {},
   "source": [
    "## Step 3: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Analyze data quality issues before preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e730346",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DATA QUALITY ANALYSIS - BEFORE PREPROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. Data types and basic info\n",
    "print(\"\\n1Ô∏è‚É£  DATA TYPES AND BASIC INFO:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "# 2. Missing values analysis\n",
    "print(\"\\n2Ô∏è‚É£  MISSING VALUES ANALYSIS:\")\n",
    "missing_count = df_raw.isnull().sum()\n",
    "missing_percent = (df_raw.isnull().sum() / len(df_raw) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_count.index,\n",
    "    'Missing_Count': missing_count.values,\n",
    "    'Missing_Percentage': missing_percent.values\n",
    "})\n",
    "print(missing_df)\n",
    "print(f\"Total Missing Values: {df_raw.isnull().sum().sum()}\")\n",
    "\n",
    "# 3. Duplicate rows\n",
    "print(f\"\\n3Ô∏è‚É£  DUPLICATE ROWS: {df_raw.duplicated().sum()} duplicates found\")\n",
    "\n",
    "# 4. Statistical summary\n",
    "print(\"\\n4Ô∏è‚É£  STATISTICAL SUMMARY:\")\n",
    "print(df_raw.describe())\n",
    "\n",
    "# 5. Data consistency issues\n",
    "print(\"\\n5Ô∏è‚É£  CONSISTENCY ISSUES:\")\n",
    "print(f\"Unique cities: {df_raw['city'].unique()}\")\n",
    "print(\"Note: Inconsistent case in city names (New York vs new york)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9249aa00",
   "metadata": {},
   "source": [
    "## Step 4: Data Cleaning Phase 1 - Handle Missing Values\n",
    "\n",
    "Different strategies for handling missing values based on data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ac114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df_raw.copy()\n",
    "\n",
    "print(\"PHASE 1: HANDLING MISSING VALUES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Strategy: Drop rows where critical columns have missing values\n",
    "print(\"\\nDropping rows with missing values in critical columns (price, square_feet)...\")\n",
    "df_cleaned = df_cleaned.dropna(subset=['price', 'square_feet'])\n",
    "print(f\"‚úÖ Rows after dropping: {len(df_cleaned)} (removed {len(df_raw) - len(df_cleaned)} rows)\")\n",
    "\n",
    "# Strategy: Impute missing numerical values with mean\n",
    "print(\"\\nImputing numerical missing values with mean...\")\n",
    "numerical_cols = ['bedrooms', 'bathrooms', 'garage']\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_cleaned[numerical_cols] = imputer.fit_transform(df_cleaned[numerical_cols])\n",
    "print(f\"‚úÖ Numerical imputation complete\")\n",
    "\n",
    "# Strategy: Fill categorical missing values with mode (most common value)\n",
    "print(\"\\nFilling categorical missing values with mode...\")\n",
    "df_cleaned['city'].fillna(df_cleaned['city'].mode()[0], inplace=True)\n",
    "print(f\"‚úÖ Categorical imputation complete\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"\\n‚úÖ RESULT: Total missing values after cleaning: {df_cleaned.isnull().sum().sum()}\")\n",
    "print(f\"   Dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb2bac",
   "metadata": {},
   "source": [
    "## Step 5: Data Cleaning Phase 2 - Remove Duplicates\n",
    "\n",
    "Identify and remove duplicate rows from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 2: REMOVING DUPLICATES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "duplicates_count = df_cleaned.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows found: {duplicates_count}\")\n",
    "\n",
    "if duplicates_count > 0:\n",
    "    print(f\"\\nSample duplicate rows:\")\n",
    "    print(df_cleaned[df_cleaned.duplicated(keep=False)].head())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    print(f\"\\n‚úÖ Duplicates removed\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")\n",
    "\n",
    "print(f\"\\n‚úÖ RESULT: Dataset shape after removing duplicates: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e201a4f",
   "metadata": {},
   "source": [
    "## Step 6: Data Cleaning Phase 3 - Detect and Treat Outliers\n",
    "\n",
    "Using IQR (Interquartile Range) method to identify and handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a886b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 3: DETECTING AND TREATING OUTLIERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def detect_outliers_iqr(data, column, multiplier=1.5):\n",
    "    \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - multiplier * IQR\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers in numerical columns\n",
    "numerical_cols_all = df_cleaned.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = {}\n",
    "\n",
    "print(f\"\\nAnalyzing numerical columns for outliers:\")\n",
    "for col in numerical_cols_all:\n",
    "    outliers, lower, upper = detect_outliers_iqr(df_cleaned, col)\n",
    "    outlier_summary[col] = len(outliers)\n",
    "    print(f\"  {col:15} - Outliers: {len(outliers):3} | Bounds: [{lower:10.2f}, {upper:10.2f}]\")\n",
    "\n",
    "total_outliers = sum(outlier_summary.values())\n",
    "print(f\"\\nTotal outlier records (before removal): {total_outliers}\")\n",
    "\n",
    "# Remove outliers for price and square_feet\n",
    "df_cleaned_no_outliers = df_cleaned.copy()\n",
    "for col in ['price', 'square_feet']:\n",
    "    _, lower, upper = detect_outliers_iqr(df_cleaned_no_outliers, col)\n",
    "    df_cleaned_no_outliers = df_cleaned_no_outliers[\n",
    "        (df_cleaned_no_outliers[col] >= lower) & (df_cleaned_no_outliers[col] <= upper)\n",
    "    ]\n",
    "\n",
    "outliers_removed = len(df_cleaned) - len(df_cleaned_no_outliers)\n",
    "print(f\"\\n‚úÖ RESULT: {outliers_removed} rows with outliers removed\")\n",
    "print(f\"   Dataset shape after outlier removal: {df_cleaned_no_outliers.shape}\")\n",
    "\n",
    "df_cleaned = df_cleaned_no_outliers  # Update for next phases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8018b4",
   "metadata": {},
   "source": [
    "## Step 7: Data Cleaning Phase 4 - Ensure Data Consistency\n",
    "\n",
    "Standardize and validate data formats and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 4: ENSURING DATA CONSISTENCY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Standardize categorical text (lowercase, strip whitespace)\n",
    "print(\"\\nStandardizing categorical values...\")\n",
    "print(f\"Before: {df_cleaned['city'].unique()}\")\n",
    "df_cleaned['city'] = df_cleaned['city'].str.lower().str.strip()\n",
    "print(f\"After:  {df_cleaned['city'].unique()}\")\n",
    "\n",
    "# Round numerical values to reasonable precision\n",
    "print(\"\\nRounding numerical values to reasonable precision...\")\n",
    "df_cleaned['bedrooms'] = df_cleaned['bedrooms'].round(0).astype(int)\n",
    "df_cleaned['bathrooms'] = df_cleaned['bathrooms'].round(1)\n",
    "df_cleaned['garage'] = df_cleaned['garage'].round(0).astype(int)\n",
    "\n",
    "# Validate data ranges\n",
    "print(\"\\nValidating data ranges...\")\n",
    "assert df_cleaned['price'].min() > 0, \"Price should be positive\"\n",
    "assert df_cleaned['square_feet'].min() > 0, \"Square feet should be positive\"\n",
    "assert df_cleaned['bedrooms'].min() >= 0, \"Bedrooms should be non-negative\"\n",
    "print(\"‚úÖ All data validations passed\")\n",
    "\n",
    "print(f\"\\n‚úÖ RESULT: Data consistency check completed\")\n",
    "print(f\"\\nCleaned dataset (first 10 rows):\")\n",
    "print(df_cleaned.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ff78f",
   "metadata": {},
   "source": [
    "## Step 8: Feature Scaling (Normalization)\n",
    "\n",
    "Scale numerical features to comparable ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ef3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 5: FEATURE SCALING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify numerical features\n",
    "numerical_features = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumerical features to scale: {numerical_features}\")\n",
    "\n",
    "# Apply StandardScaler (mean=0, std=1)\n",
    "print(\"\\n1Ô∏è‚É£  StandardScaler (mean=0, std=1):\")\n",
    "scaler_standard = StandardScaler()\n",
    "df_scaled_standard = df_cleaned.copy()\n",
    "df_scaled_standard[numerical_features] = scaler_standard.fit_transform(df_cleaned[numerical_features])\n",
    "\n",
    "print(\"\\nBefore StandardScaler:\")\n",
    "print(df_cleaned[numerical_features].describe().round(2))\n",
    "\n",
    "print(\"\\nAfter StandardScaler:\")\n",
    "print(df_scaled_standard[numerical_features].describe().round(2))\n",
    "\n",
    "# Apply MinMaxScaler (range 0-1)\n",
    "print(\"\\n2Ô∏è‚É£  MinMaxScaler (range 0-1):\")\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled_minmax = df_cleaned.copy()\n",
    "df_scaled_minmax[numerical_features] = scaler_minmax.fit_transform(df_cleaned[numerical_features])\n",
    "\n",
    "print(\"\\nAfter MinMaxScaler:\")\n",
    "print(df_scaled_minmax[numerical_features].describe().round(2))\n",
    "\n",
    "print(\"\\n‚úÖ RESULT: Feature scaling completed (using StandardScaler for further processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad500223",
   "metadata": {},
   "source": [
    "## Step 9: Categorical Feature Encoding\n",
    "\n",
    "Encode categorical variables for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d15a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 6: CATEGORICAL FEATURE ENCODING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = df_cleaned.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCategorical features to encode: {categorical_features}\")\n",
    "print(f\"Unique values: {df_cleaned['city'].unique()}\")\n",
    "\n",
    "# Method 1: Label Encoding (ordinal encoding)\n",
    "print(\"\\n1Ô∏è‚É£  LABEL ENCODING (for ordinal categories):\")\n",
    "df_label_encoded = df_cleaned.copy()\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_label_encoded[col] = le.fit_transform(df_label_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"\\n  {col}:\")\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        print(f\"    {label:20} ‚Üí {i}\")\n",
    "\n",
    "# Method 2: One-Hot Encoding (nominal encoding)\n",
    "print(\"\\n2Ô∏è‚É£  ONE-HOT ENCODING (for nominal categories):\")\n",
    "df_onehot = pd.get_dummies(df_cleaned, columns=categorical_features, drop_first=False)\n",
    "print(f\"Shape before One-Hot: {df_cleaned.shape}\")\n",
    "print(f\"Shape after One-Hot:  {df_onehot.shape}\")\n",
    "print(f\"\\nNew columns created:\")\n",
    "new_cols = [col for col in df_onehot.columns if col not in df_cleaned.columns]\n",
    "for col in new_cols:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "print(\"\\n‚úÖ RESULT: Categorical encoding completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b88e3",
   "metadata": {},
   "source": [
    "## Step 10: Create and Apply Preprocessing Pipeline\n",
    "\n",
    "Combine all preprocessing steps into a reusable pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cc6da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 7: PREPROCESSING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "print(\"\\nCreating preprocessing pipeline...\")\n",
    "\n",
    "numerical_features = df_cleaned.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df_cleaned.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse=False), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create full pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Pipeline created successfully\")\n",
    "\n",
    "# Apply pipeline\n",
    "print(\"\\nApplying pipeline to cleaned data...\")\n",
    "X_processed = preprocessing_pipeline.fit_transform(df_cleaned)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = []\n",
    "feature_names.extend(numerical_features)\n",
    "cat_features = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features)\n",
    "feature_names.extend(cat_features)\n",
    "\n",
    "# Create processed DataFrame\n",
    "df_final = pd.DataFrame(X_processed, columns=feature_names)\n",
    "\n",
    "print(f\"‚úÖ Pipeline applied successfully\")\n",
    "print(f\"\\nProcessed dataset shape: {df_final.shape}\")\n",
    "print(f\"\\nProcessed data (first 5 rows):\")\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded8cb0",
   "metadata": {},
   "source": [
    "## Step 11: Validation & Visualization\n",
    "\n",
    "Verify preprocessing quality through visualizations and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a449c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PHASE 8: VALIDATION & QUALITY METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create quality report\n",
    "print(\"\\nüìä DATA QUALITY REPORT\\n\")\n",
    "quality_report = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Total Rows',\n",
    "        'Missing Values',\n",
    "        'Duplicate Rows',\n",
    "        'Total Columns',\n",
    "        'Numerical Columns',\n",
    "        'Categorical Columns'\n",
    "    ],\n",
    "    'Before Preprocessing': [\n",
    "        df_raw.shape[0],\n",
    "        df_raw.isnull().sum().sum(),\n",
    "        df_raw.duplicated().sum(),\n",
    "        df_raw.shape[1],\n",
    "        len(df_raw.select_dtypes(include=[np.number]).columns),\n",
    "        len(df_raw.select_dtypes(include=['object']).columns)\n",
    "    ],\n",
    "    'After Preprocessing': [\n",
    "        df_final.shape[0],\n",
    "        df_final.isnull().sum().sum(),\n",
    "        df_final.duplicated().sum(),\n",
    "        df_final.shape[1],\n",
    "        df_final.shape[1],\n",
    "        0\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(quality_report.to_string(index=False))\n",
    "\n",
    "# Improvement metrics\n",
    "print(\"\\n‚úÖ IMPROVEMENTS:\")\n",
    "print(f\"  ‚Ä¢ Data quality increased: {((1 - df_final.isnull().sum().sum()/len(df_final)) * 100):.1f}%\")\n",
    "print(f\"  ‚Ä¢ Rows preserved: {(df_final.shape[0]/df_raw.shape[0] * 100):.1f}%\")\n",
    "print(f\"  ‚Ä¢ Outliers handled: {df_raw.shape[0] - df_final.shape[0]} rows removed/cleaned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6979e",
   "metadata": {},
   "source": [
    "## Step 12: Visualizations\n",
    "\n",
    "Create visualizations comparing before and after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739be97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Data Preprocessing Comparison: Before vs After', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Distribution comparison - Price\n",
    "axes[0, 0].hist(df_raw['price'], bins=30, color='red', alpha=0.7, edgecolor='black', label='Before')\n",
    "axes[0, 0].axvline(df_raw['price'].mean(), color='darkred', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0, 0].set_title('Price Distribution - Before Preprocessing', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Price ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Distribution after scaling\n",
    "axes[0, 1].hist(df_scaled_standard[numerical_features[0]], bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].axvline(0, color='darkgreen', linestyle='--', linewidth=2, label='Mean (0)')\n",
    "axes[0, 1].set_title('Price Distribution - After StandardScaler', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Scaled Price (StandardScaler)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Missing values comparison\n",
    "missing_before = [df_raw.isnull().sum().sum()]\n",
    "missing_after = [df_cleaned.isnull().sum().sum()]\n",
    "axes[1, 0].bar(['Before', 'After'], [missing_before[0], missing_after[0]], color=['red', 'green'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Missing Values: Before vs After', fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Data shape comparison\n",
    "shapes = [df_raw.shape[0], df_cleaned.shape[0]]\n",
    "axes[1, 1].bar(['Original', 'Cleaned'], shapes, color=['orange', 'blue'], alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Dataset Size: Before vs After', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Number of Rows')\n",
    "for i, v in enumerate(shapes):\n",
    "    axes[1, 1].text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizations created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cdff52",
   "metadata": {},
   "source": [
    "## Step 13: Summary & Results\n",
    "\n",
    "Summary of preprocessing steps and key outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PRACTICAL 2 - DATA PREPROCESSING: FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìã PREPROCESSING STEPS COMPLETED:\")\n",
    "print(\"  ‚úÖ Step 1: Loaded raw data (300 rows √ó 7 columns)\")\n",
    "print(\"  ‚úÖ Step 2: Identified data quality issues\")\n",
    "print(\"  ‚úÖ Step 3: Handled missing values using imputation and deletion\")\n",
    "print(\"  ‚úÖ Step 4: Removed duplicate rows\")\n",
    "print(\"  ‚úÖ Step 5: Detected and treated outliers using IQR method\")\n",
    "print(\"  ‚úÖ Step 6: Ensured data consistency (standardization, validation)\")\n",
    "print(\"  ‚úÖ Step 7: Scaled numerical features (StandardScaler)\")\n",
    "print(\"  ‚úÖ Step 8: Encoded categorical features (One-Hot Encoding)\")\n",
    "print(\"  ‚úÖ Step 9: Created reusable preprocessing pipeline\")\n",
    "print(\"  ‚úÖ Step 10: Validated preprocessing quality\")\n",
    "print(\"  ‚úÖ Step 11: Generated visualizations and metrics\")\n",
    "\n",
    "print(\"\\nüìä KEY RESULTS:\")\n",
    "print(f\"  ‚Ä¢ Original dataset: {df_raw.shape[0]} rows √ó {df_raw.shape[1]} columns\")\n",
    "print(f\"  ‚Ä¢ Cleaned dataset: {df_cleaned.shape[0]} rows √ó {df_cleaned.shape[1]} columns\")\n",
    "print(f\"  ‚Ä¢ Final dataset: {df_final.shape[0]} rows √ó {df_final.shape[1]} columns\")\n",
    "print(f\"  ‚Ä¢ Data quality: {(1 - df_final.isnull().sum().sum()/len(df_final)) * 100:.1f}% (Missing: {df_final.isnull().sum().sum()})\")\n",
    "print(f\"  ‚Ä¢ Rows retained: {(df_final.shape[0]/df_raw.shape[0] * 100):.1f}%\")\n",
    "\n",
    "print(\"\\nüéØ LEARNING OUTCOMES ACHIEVED:\")\n",
    "print(\"  ‚úÖ LO1: Identified different data quality issues\")\n",
    "print(\"  ‚úÖ LO2: Handled missing values using various strategies\")\n",
    "print(\"  ‚úÖ LO3: Detected and treated outliers\")\n",
    "print(\"  ‚úÖ LO4: Normalized and scaled numerical features\")\n",
    "print(\"  ‚úÖ LO5: Encoded categorical variables\")\n",
    "print(\"  ‚úÖ LO6: Created preprocessing pipelines\")\n",
    "print(\"  ‚úÖ LO7: Validated preprocessing quality\")\n",
    "print(\"  ‚úÖ LO8: Documented preprocessing steps\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ PRACTICAL 2 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aace77f",
   "metadata": {},
   "source": [
    "## Step 14: Submission Checklist\n",
    "\n",
    "Ensure all required elements are completed before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca596284",
   "metadata": {},
   "source": [
    "### üìù PRACTICAL 2 SUBMISSION CHECKLIST\n",
    "\n",
    "Before submitting, ensure you have completed:\n",
    "\n",
    "#### Code Execution\n",
    "- [ ] All code cells executed successfully without errors\n",
    "- [ ] All visualizations displayed correctly\n",
    "- [ ] Output shows expected results and metrics\n",
    "\n",
    "#### Learning Outcomes\n",
    "- [ ] LO1: Data quality issues identified ‚úÖ\n",
    "- [ ] LO2: Missing values handled ‚úÖ\n",
    "- [ ] LO3: Outliers detected and treated ‚úÖ\n",
    "- [ ] LO4: Features scaled ‚úÖ\n",
    "- [ ] LO5: Categorical features encoded ‚úÖ\n",
    "- [ ] LO6: Pipeline created ‚úÖ\n",
    "- [ ] LO7: Quality validated ‚úÖ\n",
    "- [ ] LO8: Steps documented ‚úÖ\n",
    "\n",
    "#### Documentation\n",
    "- [ ] Student details filled in (Name, Roll No, Date)\n",
    "- [ ] All code cells have appropriate comments\n",
    "- [ ] Results explained in text cells\n",
    "- [ ] Reflections written below\n",
    "\n",
    "#### Files to Submit\n",
    "- [ ] This notebook (`Practical_2_Complete_Notebook.ipynb`)\n",
    "- [ ] Submission template (`SUBMISSION_TEMPLATE_Practical_2.md`)\n",
    "- [ ] Screenshot of final summary\n",
    "\n",
    "---\n",
    "\n",
    "### üí≠ REFLECTIONS & LEARNINGS\n",
    "\n",
    "**1. What I learned about data quality:**\n",
    "\n",
    "Real-world data is messier than we expect. Missing values, duplicates, and outliers are common and must be handled carefully. Different strategies work for different scenarios.\n",
    "\n",
    "**2. Key challenges faced:**\n",
    "\n",
    "- Deciding which rows to delete vs impute\n",
    "- Balancing data loss with data quality\n",
    "- Understanding when to use different scaling methods\n",
    "\n",
    "**3. Important concepts understood:**\n",
    "\n",
    "- Missing data mechanisms (MCAR, MAR, MNAR)\n",
    "- Outlier detection methods (IQR, Z-score)\n",
    "- Feature scaling importance for ML algorithms\n",
    "- Categorical encoding strategies\n",
    "\n",
    "**4. How this applies to real-world ML:**\n",
    "\n",
    "Data preprocessing is often 70% of ML work. Clean data = Better models. The pipelines we created can be reused for new datasets.\n",
    "\n",
    "**5. Skills developed:**\n",
    "\n",
    "- Pandas data manipulation\n",
    "- Scikit-learn preprocessing\n",
    "- Pipeline creation and reusability\n",
    "- Data quality assessment\n",
    "- Data visualization\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for submission?** ‚úÖ Yes / ‚ùå No\n",
    "\n",
    "**Submission date:** _____________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
